{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning BERT(`distilbert-base-cased`) for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(example):\n",
    "    return {\n",
    "        'text': ' '.join(example['text'].split()[:50]),\n",
    "        'label': example['label']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "imdb = load_dataset('stanfordnlp/imdb')\n",
    "\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = DatasetDict(\n",
    "    train=imdb['train'].shuffle(5525).map(truncate),\n",
    "    val=imdb['test'].shuffle(5525).map(truncate)\n",
    ")\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"It seems Hal Hartley's films are kind of hit or miss with most audiences. This film will be no exception to that rule. Fay Grim acts as a sequel to Hartley's 'Henry Foole' from 1998. The focus this time is on Henry's ex wife (played to perfection by the always\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(imdb['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4425d92f0ff040c9bb26663d7dd54b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n",
      "{'text': \"It seems Hal Hartley's films are kind of hit or miss with most audiences. This film will be no exception to that rule. Fay Grim acts as a sequel to Hartley's 'Henry Foole' from 1998. The focus this time is on Henry's ex wife (played to perfection by the always\", 'label': 1, 'input_ids': [101, 1135, 3093, 12193, 23053, 112, 188, 2441, 1132, 1912, 1104, 1855, 1137, 5529, 1114, 1211, 9569, 119, 1188, 1273, 1209, 1129, 1185, 5856, 1106, 1115, 3013, 119, 26630, 144, 10205, 4096, 1112, 170, 8047, 1106, 23053, 112, 188, 112, 1985, 21935, 1162, 112, 1121, 1772, 119, 1109, 2817, 1142, 1159, 1110, 1113, 1985, 112, 188, 4252, 1676, 113, 1307, 1106, 17900, 1118, 1103, 1579, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n",
      "----------------------------------------------------------------------------------------------------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n",
      "{'labels': 1, 'input_ids': [101, 1135, 3093, 12193, 23053, 112, 188, 2441, 1132, 1912, 1104, 1855, 1137, 5529, 1114, 1211, 9569, 119, 1188, 1273, 1209, 1129, 1185, 5856, 1106, 1115, 3013, 119, 26630, 144, 10205, 4096, 1112, 170, 8047, 1106, 23053, 112, 188, 112, 1985, 21935, 1162, 112, 1121, 1772, 119, 1109, 2817, 1142, 1159, 1110, 1113, 1985, 112, 188, 4252, 1676, 113, 1307, 1106, 17900, 1118, 1103, 1579, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "dataset = imdb.map(\n",
    "    lambda example: tokenizer(example['text'], padding=True, truncation=True),\n",
    "    batched=True,\n",
    "    batch_size=32\n",
    ")\n",
    "print(dataset)\n",
    "print(dataset['train'][0])\n",
    "print('-' * 100)\n",
    "\n",
    "dataset = dataset.remove_columns(['text'])\n",
    "print(dataset)\n",
    "print('-' * 100)\n",
    "\n",
    "dataset = dataset.rename_column('label', 'labels')\n",
    "print(dataset)\n",
    "print(dataset['train'][0])\n",
    "\n",
    "dataset.set_format('torch')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset['train'], batch_size=32)\n",
    "eval_loader = DataLoader(dataset['val'], batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
    "model.to('cuda')\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = len(train_loader)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-3, weight_decay=0.01)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089b0a1809b8421bb17e06763168d6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n",
      "validation loss=0.39201114532511555\n",
      "epoch=1\n",
      "validation loss=0.39201114532511555\n",
      "epoch=2\n",
      "validation loss=0.39201114532511555\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'epoch={epoch}')\n",
    "    # training\n",
    "    model.train()\n",
    "    for batch_id, batch in enumerate(train_loader):\n",
    "        # batch = ([text1, text2], [0, 1])\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        output = model(**batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output.loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_id, batch in enumerate(eval_loader):\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model(**batch)\n",
    "        total_loss += output.loss.item()\n",
    "\n",
    "    avg_val_loss = total_loss / len(eval_loader)\n",
    "    print(f'validation loss={avg_val_loss}')\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # Ê£ÄÊü•ÁõÆÂΩïÊòØÂê¶Â≠òÂú®Ôºå‰∏çÂ≠òÂú®ÂàôÂàõÂª∫\n",
    "        if not os.path.exists('checkpoints/'):\n",
    "            os.makedirs('checkpoints/')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss,\n",
    "        }, f\"checkpoints/epoch_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0656, -0.1659]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "tensor(0)\n",
      "\n",
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# To load our saved model, we can pass the path to the checkpoint into the `from_pretrained` method:\n",
    "test_str = \"wtf\"\n",
    "\n",
    "# Âä†ËΩΩ‰øùÂ≠òÁöÑÊ®°Âûã\n",
    "checkpoint = torch.load(\"checkpoints/epoch_0.pt\", map_location='cuda')\n",
    "# ‰ªéÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂàùÂßãÂåñ\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\")\n",
    "# Â∞Ü‰øùÂ≠òÁöÑ state_dict Âä†ËΩΩÂà∞Ê®°Âûã‰∏≠\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "\n",
    "model_inputs = tokenizer(test_str, return_tensors=\"pt\")\n",
    "print(model(**model_inputs).logits)\n",
    "print()\n",
    "\n",
    "prediction = torch.argmax(model(**model_inputs).logits)\n",
    "print(prediction)\n",
    "print()\n",
    "\n",
    "print([\"NEGATIVE\", \"POSITIVE\"][prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_4735/3918396320.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='2346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/2346 02:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.473300</td>\n",
       "      <td>0.411927</td>\n",
       "      <td>0.808920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.313600</td>\n",
       "      <td>0.393210</td>\n",
       "      <td>0.825560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.431751</td>\n",
       "      <td>0.825040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2346, training_loss=0.3349299353610118, metrics={'train_runtime': 146.1567, 'train_samples_per_second': 513.148, 'train_steps_per_second': 16.051, 'total_flos': 2532991921825536.0, 'train_loss': 0.3349299353610118, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
    "\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"sample_hf_trainer\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=5525\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # calculates the accuracy\n",
    "    return {\"accuracy\": np.mean(predictions == labels)}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=arguments,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['val'], # change to test when you do your final evaluation!\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[ 0.15588348, -0.03970203]], dtype=float32), label_ids=None, metrics={'test_runtime': 0.0156, 'test_samples_per_second': 64.124, 'test_steps_per_second': 64.124})\n",
      "\n",
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "test_encoding = tokenizer(\"im sad\", padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Â∞ÜÂ≠óÂÖ∏ÂåÖË£ÖÂà∞‰∏Ä‰∏™ÂàóË°®‰∏≠\n",
    "test_dataset = [ {k: v.squeeze(0) for k, v in test_encoding.items()} ]\n",
    "prediction_output = trainer.predict(test_dataset)\n",
    "print(prediction_output)\n",
    "print()\n",
    "\n",
    "predicted_indices = np.argmax(prediction_output.predictions, axis=-1)\n",
    "# Â¶ÇÊûúÂè™È¢ÑÊµã‰∏Ä‰∏™Ê†∑Êú¨ÔºåÂèñÁ¨¨‰∏Ä‰∏™Á¥¢Âºï\n",
    "prediction = predicted_indices[0]\n",
    "print([\"NEGATIVE\", \"POSITIVE\"][prediction])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
