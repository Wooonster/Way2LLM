{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse MoE\n",
    "\n",
    "MoE 选择 topK 个专家，对选出的 K 个专家的输出进行加权求和，并把输入样本变成 LLM 中真实的输入 shape -> (batch_size, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        expert_num,\n",
    "        top_k,\n",
    "        shared_experts_num=2,\n",
    "    ):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.expert_num = expert_num\n",
    "        self.top_k = top_k\n",
    "        self.shared_experts_num = shared_experts_num\n",
    "\n",
    "class MoERouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        self.gate = nn.Linear(config.hidden_dim, config.expert_num)\n",
    "        # select top k experts\n",
    "        self.top_k = config.top_k\n",
    "        self.expert_num = config.expert_num\n",
    "    \n",
    "    def forward(self, x):\n",
    "        router_logits = self.gate(x)  # (batch_size * seq_len, expert_num)\n",
    "        # 计算每个专家的概率\n",
    "        router_probs = F.softmax(router_logits, dim=1, dtype=torch.float32)\n",
    "        # 计算 top k 个专家的输出、索引\n",
    "        router_weights, selected_indices = torch.topk(\n",
    "            router_probs,\n",
    "            self.top_k,\n",
    "            dim=-1\n",
    "        )  # (batch_size * seq_len, top_k)\n",
    "\n",
    "        # 重新归一化\n",
    "        router_weights /= router_weights.sum(dim=-1, keepdim=True) \n",
    "        router_weights = router_weights.to(x.dtype)\n",
    "\n",
    "        # 生成 mask\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        expert_mask = F.one_hot(\n",
    "            selected_indices,\n",
    "            num_classes=self.expert_num\n",
    "        )  # -> (batch_size * seq_len, top_k, expert_num)\n",
    "\n",
    "        expert_mask = expert_mask.permute(2, 1, 0)\n",
    "        # -> (expert_num, top_k, batch_size * seq_len)\n",
    "\n",
    "        return router_logits, router_weights, selected_indices, expert_mask\n",
    "\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.expert_num = config.expert_num\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "        # initialize experts\n",
    "        self.experts = nn.ModuleList(\n",
    "            BasicExpert(config.hidden_dim, config.hidden_dim)\n",
    "            for _ in range(config.expert_num)\n",
    "        )\n",
    "\n",
    "        self.router = MoERouter(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (b, seq_len, hidden_dim)\n",
    "        batch_size, seq_len, hidden_dim = x.size() \n",
    "\n",
    "        # 对 token 维度计算, x -reshape-> (batch * seq_len, hidden_dim)\n",
    "        hidden_state = x.view(-1, hidden_dim)\n",
    "\n",
    "        router_logits, router_weights, selected_indices, expert_mask = self.router(hidden_state)\n",
    "        # expert_mask -> (expert_num, top_k, batch_size * seq_len)\n",
    "\n",
    "        # final hidden states -> (batch_size * seq_len, hidden_dim)\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * seq_len, hidden_dim),\n",
    "            device=hidden_state.device\n",
    "        )\n",
    "\n",
    "        # 遍历每个 expert\n",
    "        # 把该 expert 的 token 的 hidden_states 加到 final_hidden_states 上\n",
    "        # token 总数是 batch_size * seq_len\n",
    "        for expert_idx in range(self.expert_num):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            # 选择该 expert 的 mask (expert_num, top_k, batch_size * seq_len)\n",
    "            cur_expert_mask = expert_mask[expert_idx]\n",
    "            # -> (top_k, batch_size * seq_len)\n",
    "\n",
    "            idx, top_x = torch.where(cur_expert_mask > 0)\n",
    "            '''\n",
    "            idx: 选择的（topK 中）第 i 个 expert；用于选择 weights\n",
    "            top_x: 是 token 在 batch * seq_len 中的索引 （1 维的值）；选择 hidden_states\n",
    "            '''\n",
    "\n",
    "            cur_state = hidden_state.unsqueeze(0)[:, top_x, :].reshape(-1, hidden_dim)\n",
    "            # unsqueeze(0) -> (1, batch_size * seq_len, hidden_dim)\n",
    "            cur_state = expert_layer(cur_state)\n",
    "            cur_token_router_weight = router_weights[top_x, idx]\n",
    "            # -> (selected_token_num)\n",
    "            cur_token_router_weight = cur_token_router_weight.unsqueeze(-1)\n",
    "            # -> (selected_token_num, 1)\n",
    "\n",
    "            cur_hidden_states = cur_state * cur_token_router_weight\n",
    "            # -> (selected_token_num, hidden_dim)\n",
    "\n",
    "            final_hidden_states.index_add_(\n",
    "                0,\n",
    "                top_x,\n",
    "                cur_hidden_states.to(final_hidden_states.dtype)\n",
    "            )\n",
    "        \n",
    "        # 还原到原来的 shape\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)\n",
    "        return final_hidden_states, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
