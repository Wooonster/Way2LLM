{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNBasicExpert(nn.Module):\n",
    "    def __init__(self, feature_in, feature_out):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(feature_in, feature_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMoE(nn.Module):\n",
    "    def __init__(self, feature_in, feature_out, num_experts):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList(\n",
    "            FFNBasicExpert(feature_in, feature_out) \n",
    "            for _ in range(num_experts)\n",
    "        )\n",
    "        # gate 即选一个 expert 的权重\n",
    "        # 将 input 送入 gate，分给 num_experts 个 expert\n",
    "        # 得到 num_experts 个权重\n",
    "        self.gate = nn.Linear(feature_in, num_experts)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, feature_in), feature_in or hidden_dim\n",
    "        expert_weights = self.gate(x)  # (batch_size, num_experts)\n",
    "        expert_out_list = [\n",
    "            expert(x).unsqueeze(1) for expert in self.experts\n",
    "        ]  # 每个 expert 输出一个 (batch_size, feature_out) \n",
    "\n",
    "        expert_output = torch.concat(\n",
    "            expert_out_list, dim=1\n",
    "        )  # -> (b, num_experts, feature_out)\n",
    "\n",
    "        ' 加权 '\n",
    "        # expert_weights -> (b, num_experts)\n",
    "        # expert_weights = F.softmax(expert_weights, dim=1)\n",
    "        expert_weights = expert_weights.unsqueeze(1)  # -> (b, 1, num_experts)\n",
    "        \n",
    "        output = expert_weights @ expert_output  # -> (b, 1, feature_out)\n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4, 512)  # (batch_size, feature_in)\n",
    "basic_moe = BasicMoE(512, 128, 5)  # (feature_in, feature_out, num_experts)\n",
    "output = basic_moe(x)\n",
    "print(output.shape)  # (4, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse MoE\n",
    "\n",
    "MoE 选择 topK 个专家，对选出的 K 个专家的输出进行加权求和，并把输入样本变成 LLM 中真实的输入 shape -> (batch_size, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        expert_num,\n",
    "        top_k,\n",
    "        shared_experts_num=2,\n",
    "    ):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.expert_num = expert_num\n",
    "        self.top_k = top_k\n",
    "        self.shared_experts_num = shared_experts_num\n",
    "\n",
    "class MoERouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(config.hidden_dim, config.expert_num)\n",
    "        # select top k experts\n",
    "        self.top_k = config.top_k\n",
    "        self.expert_num = config.expert_num\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 计算路由 logits\n",
    "        router_logits = self.gate(x)  # (batch_size * seq_len, expert_num)\n",
    "        # 计算每个专家的概率\n",
    "        router_probs = F.softmax(router_logits, dim=1, dtype=torch.float32)\n",
    "        # 计算 top k 个专家的输出、索引\n",
    "        router_weights, selected_indices = torch.topk(\n",
    "            router_probs,\n",
    "            self.top_k,\n",
    "            dim=-1\n",
    "        )  # (batch_size * seq_len, top_k)\n",
    "\n",
    "        # 重新归一化\n",
    "        router_weights /= router_weights.sum(dim=-1, keepdim=True) \n",
    "        router_weights = router_weights.to(x.dtype)\n",
    "\n",
    "        # 生成专家 mask\n",
    "        expert_mask = F.one_hot(\n",
    "            selected_indices,\n",
    "            num_classes=self.expert_num\n",
    "        )  # -> (batch_size * seq_len, top_k, expert_num)\n",
    "        expert_mask = expert_mask.permute(2, 1, 0)\n",
    "        # -> (expert_num, top_k, batch_size * seq_len)\n",
    "\n",
    "        return router_logits, router_weights, selected_indices, expert_mask\n",
    "\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    # 稀疏 MOE 模型，这里每一个 token 都会过 topk 个专家\n",
    "    # 得到对应token 的 hidden_embeddings\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.expert_num = config.expert_num\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "        # initialize experts\n",
    "        self.experts = nn.ModuleList(\n",
    "            FFNBasicExpert(config.hidden_dim, config.hidden_dim)\n",
    "            for _ in range(config.expert_num)\n",
    "        )\n",
    "\n",
    "        self.router = MoERouter(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (b, seq_len, hidden_dim)\n",
    "        batch_size, seq_len, hidden_dim = x.size() \n",
    "\n",
    "        # 合并前两个维度，因为不是 Sample 维度了，而是 token 维度\n",
    "        # 对 token 维度计算, x -reshape-> (batch * seq_len, hidden_dim)\n",
    "        hidden_state = x.view(-1, hidden_dim)\n",
    "\n",
    "        router_logits, router_weights, selected_indices, expert_mask = self.router(hidden_state)\n",
    "        # selected_indices -> (batch_size * seq_len, top_k)\n",
    "        # expert_mask -> (expert_num, top_k, batch_size * seq_len)\n",
    "\n",
    "        # 初始化 final hidden states -> (batch_size * seq_len, hidden_dim)\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * seq_len, hidden_dim),\n",
    "            device=hidden_state.device\n",
    "        )\n",
    "\n",
    "        # 遍历每个 expert\n",
    "        # 把该 expert 的 token 的 hidden_states 加到 final_hidden_states 上\n",
    "        # token 总数是 batch_size * seq_len\n",
    "        for expert_idx in range(self.expert_num):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            # 选择该 expert 的 mask (expert_num, top_k, batch_size * seq_len)\n",
    "            cur_expert_mask = expert_mask[expert_idx]\n",
    "            # -> (top_k, batch_size * seq_len)\n",
    "\n",
    "            idx, top_x = torch.where(cur_expert_mask > 0)\n",
    "            '''\n",
    "            idx: 选择的（topK 中）第 i 个 expert；用于选择 weights\n",
    "            top_x: 是 token 在 batch * seq_len 中的索引 （1 维的值）；选择 hidden_states\n",
    "            '''\n",
    "\n",
    "            # hidden_states 的 shape 是 (b * s, hidden_dim)\n",
    "            # 需要取到 top_x 对应的 hidden_states\n",
    "            cur_state = hidden_state.unsqueeze(0)[:, top_x, :].reshape(-1, hidden_dim)\n",
    "            # unsqueeze(0) -> (1, batch_size * seq_len, hidden_dim)\n",
    "            # cur_state -> (selected_token_num, hidden_dim)\n",
    "            cur_state = expert_layer(cur_state)\n",
    "            cur_token_router_weight = router_weights[top_x, idx]\n",
    "            # -> (selected_token_num)\n",
    "            cur_token_router_weight = cur_token_router_weight.unsqueeze(-1)\n",
    "            # -> (selected_token_num, 1)\n",
    "\n",
    "            cur_hidden_states = cur_state * cur_token_router_weight\n",
    "            # -> (selected_token_num, hidden_dim)\n",
    "\n",
    "            final_hidden_states.index_add_(\n",
    "                0,\n",
    "                top_x,\n",
    "                cur_hidden_states.to(final_hidden_states.dtype)\n",
    "            )\n",
    "        \n",
    "        # 还原到原来的 shape\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)\n",
    "        return final_hidden_states, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 16]), torch.Size([8, 2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 4, 16)\n",
    "config = MoEConfig(16, 2, 2)\n",
    "token_level_moe = SparseMoE(config)\n",
    "out = token_level_moe(x)\n",
    "\n",
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShareExpert SparseMoE\n",
    "\n",
    "相较于 SparseMoE 多了 shared experts 模型 ，即共享 token，所有 token 都经过这个 shared experts 模型，然后每个 token 会 用计算的 router 权重，选择 topK 个专家，再与共享的专家输出一起加权求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedExpertMoE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.routed_experts_moe = SparseMoE(config)\n",
    "        self.shared_experts = nn.ModuleList([\n",
    "            FFNBasicExpert(\n",
    "                config.hidden_dim, config.hidden_dim\n",
    "            ) for _ in range(config.shared_experts_num)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x -> (batch_size, seq_len, hidden_dim)\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        \n",
    "        # 首先过 moe 模型\n",
    "        sparse_moe_out, router_logits = self.routed_experts_moe(x)\n",
    "\n",
    "        # 针对 x 的每一个, 过 shared experts\n",
    "        shared_experts_output_list = [\n",
    "            expert(x) for expert in self.shared_experts\n",
    "        ]\n",
    "\n",
    "        shared_experts_output = torch.stack(\n",
    "            shared_experts_output_list,\n",
    "            dim=0\n",
    "        ).sum(dim=0)  # -> (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        output = shared_experts_output + sparse_moe_out\n",
    "\n",
    "        return output, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 16]), torch.Size([8, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 4, 16)\n",
    "config = MoEConfig(16, 2, 2)\n",
    "shared_experts_moe = SharedExpertMoE(config)\n",
    "out = shared_experts_moe(x)\n",
    "\n",
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MOEConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 87\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Aux: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maux_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Run the training test\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43mtest_moe_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 52\u001b[0m, in \u001b[0;36mtest_moe_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Initialize model and optimizer\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mMOEConfig\u001b[49m(hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim, \n\u001b[1;32m     53\u001b[0m                   expert_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     54\u001b[0m                   top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     55\u001b[0m                   shared_experts_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m ShareExpertMOE(config)\n\u001b[1;32m     57\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MOEConfig' is not defined"
     ]
    }
   ],
   "source": [
    "# 测试， loss 部分为 deepseek 生成；\n",
    "\n",
    "def switch_load_balancing_loss(router_logits: torch.Tensor, num_experts: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算 Switch Transformers 的负载均衡损失\n",
    "    \n",
    "    Args:\n",
    "        router_logits: shape [batch_size * sequence_length, num_experts]\n",
    "        num_experts: 专家数量\n",
    "    \n",
    "    Returns:\n",
    "        total_loss: 总损失 = auxiliary_loss + z_loss\n",
    "    \"\"\"\n",
    "    # 计算路由概率\n",
    "    router_probs = torch.softmax(router_logits, dim=-1)  # [b*s, num_experts]\n",
    "    \n",
    "    # 获取每个token的最优专家\n",
    "    _, selected_experts = torch.topk(router_probs, k=2, dim=-1)  # [b*s]\n",
    "    \n",
    "    # 创建one-hot矩阵表示选中的专家\n",
    "    mask = torch.nn.functional.one_hot(selected_experts, num_experts).float()  # [b*s, num_experts]\n",
    "    \n",
    "    # 计算每个专家的期望负载 (理想情况下应该是 1/num_experts)\n",
    "    expected_load = torch.ones_like(router_probs) / num_experts\n",
    "    \n",
    "    # 计算实际负载 (每个专家处理的token数量除以总token数量)\n",
    "    # 在batch维度上计算平均值\n",
    "    actual_load = mask.mean(dim=0)  # [num_experts]\n",
    "    \n",
    "    # 计算auxiliary loss\n",
    "    # 这会惩罚负载分布与期望负载的差异\n",
    "    aux_loss = torch.sum(actual_load * router_probs.mean(dim=0)) * num_experts\n",
    "    \n",
    "    # 计算z_loss (可选)\n",
    "    # 这会惩罚过大的路由logits\n",
    "    z_loss = torch.mean(torch.square(router_logits))\n",
    "    z_loss_weight = 0.001  # 可调整的超参数\n",
    "    \n",
    "    # 总损失\n",
    "    total_loss = aux_loss + z_loss * z_loss_weight\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def test_moe_training():\n",
    "    # Create a simple dataset\n",
    "    batch_size = 32\n",
    "    seq_len = 16\n",
    "    hidden_dim = 32\n",
    "    num_batches = 100\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    config = MOEConfig(hidden_dim=hidden_dim, \n",
    "                      expert_number=4,\n",
    "                      top_k=2,\n",
    "                      shared_experts_number=2)\n",
    "    model = ShareExpertMOE(config)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for batch in range(num_batches):\n",
    "        # Generate random input data\n",
    "        x = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "        target = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, router_logits = model(x)\n",
    "\n",
    "        # Compute losses\n",
    "        # MSE loss for prediction\n",
    "        mse_loss = F.mse_loss(output, target)\n",
    "        \n",
    "        aux_loss = switch_load_balancing_loss(router_logits, config.expert_number)\n",
    "        # Combined loss\n",
    "        total_loss = mse_loss + 0.01 * aux_loss\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            print(f\"Batch {batch}, Loss: {total_loss.item():.4f} \"\n",
    "                  f\"(MSE: {mse_loss.item():.4f}, Aux: {aux_loss.item():.4f})\")\n",
    "\n",
    "# Run the training test\n",
    "test_moe_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
